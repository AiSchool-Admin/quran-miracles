#!/usr/bin/env python3
"""ØªØ­ÙˆÙŠÙ„ Ù…Ø¬Ù„Ø¯Ø§Øª ØªÙØ³ÙŠØ± Ø§Ù„Ø´Ø¹Ø±Ø§ÙˆÙŠ (17 Ù…Ø¬Ù„Ø¯Ø§Ù‹ PDF) Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø¢ÙŠØ§Øª.

Ù…ØµØ§Ø¯Ø± PDF:
- shamela.ws (Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©) â€” Ø§Ù„Ø£ÙƒØ«Ø± Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©
- Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ©

Ù…Ù„Ø§Ø­Ø¸Ø© Ù…Ù‡Ù…Ø©:
Ø§Ù„ØªÙØ³ÙŠØ± Ù…Ù†Ù‚ÙˆÙ„ Ù…Ù† Ø´Ø±Ø­ Ø´ÙÙ‡ÙŠ (Ù…Ø­Ø§Ø¶Ø±Ø§Øª Ù…Ø³Ø¬Ù„Ø©) â€”
ÙŠÙÙ†Ø³ÙØ¨ Ø¨ØµÙŠØºØ©: "Ù‚Ø§Ù„ Ø§Ù„Ø´Ø¹Ø±Ø§ÙˆÙŠ ÙÙŠ Ø®ÙˆØ§Ø·Ø±Ù‡..."

Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
    python data/pipelines/shaarawy_pdf_pipeline.py --pdf-dir data/shaarawy/
    python data/pipelines/shaarawy_pdf_pipeline.py --pdf-dir data/shaarawy/ --dry-run
    python data/pipelines/shaarawy_pdf_pipeline.py --pdf-dir data/shaarawy/ --volume 1
"""

import argparse
import asyncio
import json
import re
import unicodedata
from pathlib import Path
from typing import Generator

try:
    import pdfplumber
except ImportError:
    pdfplumber = None

try:
    from camel_tools.utils.normalize import normalize_unicode
except ImportError:
    def normalize_unicode(text: str) -> str:
        return unicodedata.normalize("NFC", text)


# â”€â”€ Ø³ÙˆØ± Ø§Ù„Ù‚Ø±Ø¢Ù† (Ù„Ù„Ø±Ø¨Ø·) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SURAH_NAMES = [
    "Ø§Ù„ÙØ§ØªØ­Ø©", "Ø§Ù„Ø¨Ù‚Ø±Ø©", "Ø¢Ù„ Ø¹Ù…Ø±Ø§Ù†", "Ø§Ù„Ù†Ø³Ø§Ø¡", "Ø§Ù„Ù…Ø§Ø¦Ø¯Ø©",
    "Ø§Ù„Ø£Ù†Ø¹Ø§Ù…", "Ø§Ù„Ø£Ø¹Ø±Ø§Ù", "Ø§Ù„Ø£Ù†ÙØ§Ù„", "Ø§Ù„ØªÙˆØ¨Ø©", "ÙŠÙˆÙ†Ø³",
    "Ù‡ÙˆØ¯", "ÙŠÙˆØ³Ù", "Ø§Ù„Ø±Ø¹Ø¯", "Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ…", "Ø§Ù„Ø­Ø¬Ø±",
    "Ø§Ù„Ù†Ø­Ù„", "Ø§Ù„Ø¥Ø³Ø±Ø§Ø¡", "Ø§Ù„ÙƒÙ‡Ù", "Ù…Ø±ÙŠÙ…", "Ø·Ù‡",
    "Ø§Ù„Ø£Ù†Ø¨ÙŠØ§Ø¡", "Ø§Ù„Ø­Ø¬", "Ø§Ù„Ù…Ø¤Ù…Ù†ÙˆÙ†", "Ø§Ù„Ù†ÙˆØ±", "Ø§Ù„ÙØ±Ù‚Ø§Ù†",
    "Ø§Ù„Ø´Ø¹Ø±Ø§Ø¡", "Ø§Ù„Ù†Ù…Ù„", "Ø§Ù„Ù‚ØµØµ", "Ø§Ù„Ø¹Ù†ÙƒØ¨ÙˆØª", "Ø§Ù„Ø±ÙˆÙ…",
    "Ù„Ù‚Ù…Ø§Ù†", "Ø§Ù„Ø³Ø¬Ø¯Ø©", "Ø§Ù„Ø£Ø­Ø²Ø§Ø¨", "Ø³Ø¨Ø£", "ÙØ§Ø·Ø±",
    "ÙŠØ³", "Ø§Ù„ØµØ§ÙØ§Øª", "Øµ", "Ø§Ù„Ø²Ù…Ø±", "ØºØ§ÙØ±",
    "ÙØµÙ„Øª", "Ø§Ù„Ø´ÙˆØ±Ù‰", "Ø§Ù„Ø²Ø®Ø±Ù", "Ø§Ù„Ø¯Ø®Ø§Ù†", "Ø§Ù„Ø¬Ø§Ø«ÙŠØ©",
    "Ø§Ù„Ø£Ø­Ù‚Ø§Ù", "Ù…Ø­Ù…Ø¯", "Ø§Ù„ÙØªØ­", "Ø§Ù„Ø­Ø¬Ø±Ø§Øª", "Ù‚",
    "Ø§Ù„Ø°Ø§Ø±ÙŠØ§Øª", "Ø§Ù„Ø·ÙˆØ±", "Ø§Ù„Ù†Ø¬Ù…", "Ø§Ù„Ù‚Ù…Ø±", "Ø§Ù„Ø±Ø­Ù…Ù†",
    "Ø§Ù„ÙˆØ§Ù‚Ø¹Ø©", "Ø§Ù„Ø­Ø¯ÙŠØ¯", "Ø§Ù„Ù…Ø¬Ø§Ø¯Ù„Ø©", "Ø§Ù„Ø­Ø´Ø±", "Ø§Ù„Ù…Ù…ØªØ­Ù†Ø©",
    "Ø§Ù„ØµÙ", "Ø§Ù„Ø¬Ù…Ø¹Ø©", "Ø§Ù„Ù…Ù†Ø§ÙÙ‚ÙˆÙ†", "Ø§Ù„ØªØºØ§Ø¨Ù†", "Ø§Ù„Ø·Ù„Ø§Ù‚",
    "Ø§Ù„ØªØ­Ø±ÙŠÙ…", "Ø§Ù„Ù…Ù„Ùƒ", "Ø§Ù„Ù‚Ù„Ù…", "Ø§Ù„Ø­Ø§Ù‚Ø©", "Ø§Ù„Ù…Ø¹Ø§Ø±Ø¬",
    "Ù†ÙˆØ­", "Ø§Ù„Ø¬Ù†", "Ø§Ù„Ù…Ø²Ù…Ù„", "Ø§Ù„Ù…Ø¯Ø«Ø±", "Ø§Ù„Ù‚ÙŠØ§Ù…Ø©",
    "Ø§Ù„Ø¥Ù†Ø³Ø§Ù†", "Ø§Ù„Ù…Ø±Ø³Ù„Ø§Øª", "Ø§Ù„Ù†Ø¨Ø£", "Ø§Ù„Ù†Ø§Ø²Ø¹Ø§Øª", "Ø¹Ø¨Ø³",
    "Ø§Ù„ØªÙƒÙˆÙŠØ±", "Ø§Ù„Ø§Ù†ÙØ·Ø§Ø±", "Ø§Ù„Ù…Ø·ÙÙÙŠÙ†", "Ø§Ù„Ø§Ù†Ø´Ù‚Ø§Ù‚", "Ø§Ù„Ø¨Ø±ÙˆØ¬",
    "Ø§Ù„Ø·Ø§Ø±Ù‚", "Ø§Ù„Ø£Ø¹Ù„Ù‰", "Ø§Ù„ØºØ§Ø´ÙŠØ©", "Ø§Ù„ÙØ¬Ø±", "Ø§Ù„Ø¨Ù„Ø¯",
    "Ø§Ù„Ø´Ù…Ø³", "Ø§Ù„Ù„ÙŠÙ„", "Ø§Ù„Ø¶Ø­Ù‰", "Ø§Ù„Ø´Ø±Ø­", "Ø§Ù„ØªÙŠÙ†",
    "Ø§Ù„Ø¹Ù„Ù‚", "Ø§Ù„Ù‚Ø¯Ø±", "Ø§Ù„Ø¨ÙŠÙ†Ø©", "Ø§Ù„Ø²Ù„Ø²Ù„Ø©", "Ø§Ù„Ø¹Ø§Ø¯ÙŠØ§Øª",
    "Ø§Ù„Ù‚Ø§Ø±Ø¹Ø©", "Ø§Ù„ØªÙƒØ§Ø«Ø±", "Ø§Ù„Ø¹ØµØ±", "Ø§Ù„Ù‡Ù…Ø²Ø©", "Ø§Ù„ÙÙŠÙ„",
    "Ù‚Ø±ÙŠØ´", "Ø§Ù„Ù…Ø§Ø¹ÙˆÙ†", "Ø§Ù„ÙƒÙˆØ«Ø±", "Ø§Ù„ÙƒØ§ÙØ±ÙˆÙ†", "Ø§Ù„Ù†ØµØ±",
    "Ø§Ù„Ù…Ø³Ø¯", "Ø§Ù„Ø¥Ø®Ù„Ø§Øµ", "Ø§Ù„ÙÙ„Ù‚", "Ø§Ù„Ù†Ø§Ø³",
]


class ShaarawyPDFPipeline:
    """ØªØ­ÙˆÙŠÙ„ Ù…Ø¬Ù„Ø¯Ø§Øª ØªÙØ³ÙŠØ± Ø§Ù„Ø´Ø¹Ø±Ø§ÙˆÙŠ Ù…Ù† PDF Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø¢ÙŠØ§Øª."""

    VERSE_PATTERNS = [
        r'Ù‚ÙˆÙ„Ù‡ ØªØ¹Ø§Ù„Ù‰[:\s]*[ï´¿Â«"](.*?)[ï´¾Â»"]',
        r'ï´¿(.*?)ï´¾',
        r'Ø§Ù„Ø¢ÙŠØ©\s+\((\d+)\)\s+Ù…Ù†\s+Ø³ÙˆØ±Ø©\s+(.+)',
        r'Ø³ÙˆØ±Ø©\s+([\u0600-\u06FF]+)\s+[\-â€“:]\s*Ø§Ù„Ø¢ÙŠØ©\s+(\d+)',
        r'\{(.*?)\}',
    ]

    def __init__(
        self,
        pdf_dir: str | Path,
        output_dir: str | Path | None = None,
        quran_data_dir: str | Path | None = None,
    ):
        self.pdf_dir = Path(pdf_dir)
        self.output_dir = Path(output_dir or self.pdf_dir.parent / "shaarawy_extracted")
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø±Ø¢Ù† Ù„Ù„Ø±Ø¨Ø·
        self.quran_dir = Path(quran_data_dir or self.pdf_dir.parent.parent / "data" / "quran")
        self.verse_index = self._build_verse_index()

    def _build_verse_index(self) -> dict[str, dict]:
        """Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ Ù„Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ© Ù…Ù† Ù…Ù„ÙØ§Øª JSON."""
        index: dict[str, dict] = {}

        complete_file = self.quran_dir / "quran_complete.json"
        if not complete_file.exists():
            print("âš ï¸ quran_complete.json not found â€” verse linking will be limited")
            return index

        try:
            verses = json.loads(complete_file.read_text(encoding="utf-8"))
            for v in verses:
                key = f"{v['surah_number']}:{v['verse_number']}"
                index[key] = v
                # ÙÙ‡Ø±Ø³Ø© Ø¨Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ø¸ÙŠÙ Ø£ÙŠØ¶Ø§Ù‹ (Ø£ÙˆÙ„ 30 Ø­Ø±Ù)
                clean = v.get("text_clean", "")
                if clean and len(clean) > 10:
                    index[clean[:30]] = v
        except Exception as e:
            print(f"âš ï¸ Failed to load Quran data: {e}")

        return index

    def process_all_volumes(self, volume: int | None = None) -> dict:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª (Ø£Ùˆ Ù…Ø¬Ù„Ø¯ ÙˆØ§Ø­Ø¯ Ù…Ø­Ø¯Ø¯)."""
        if pdfplumber is None:
            return {
                "error": "pdfplumber not installed. Run: pip install pdfplumber",
                "processed": 0,
            }

        results = {"processed": 0, "failed": 0, "total_chunks": 0, "linked": 0}

        if volume:
            pdf_files = list(self.pdf_dir.glob(f"*vol*{volume}*.pdf"))
            if not pdf_files:
                pdf_files = list(self.pdf_dir.glob(f"*{volume}*.pdf"))
        else:
            pdf_files = sorted(self.pdf_dir.glob("*.pdf"))

        if not pdf_files:
            print(f"âš ï¸ Ù„Ø§ Ù…Ù„ÙØ§Øª PDF ÙÙŠ {self.pdf_dir}")
            return results

        for pdf_path in pdf_files:
            vol_num = self._extract_volume_number(pdf_path)
            print(f"\nğŸ“– Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¬Ù„Ø¯ {vol_num}: {pdf_path.name}")

            try:
                chunks = list(self._extract_chunks(pdf_path))
                linked = self._link_to_verses(chunks)

                # Ø­ÙØ¸
                out_path = self.output_dir / f"shaarawy_vol_{vol_num:02d}.json"
                out_path.write_text(
                    json.dumps(
                        {
                            "volume": vol_num,
                            "source_file": pdf_path.name,
                            "total_chunks": len(chunks),
                            "linked_count": len(linked),
                            "chunks": linked,
                        },
                        ensure_ascii=False,
                        indent=2,
                    ),
                    encoding="utf-8",
                )

                results["processed"] += 1
                results["total_chunks"] += len(chunks)
                results["linked"] += len(linked)
                print(f"  âœ… {len(chunks)} Ù…Ù‚Ø·Ø¹ â†’ {len(linked)} Ù…Ø±ØªØ¨Ø· Ø¨Ø¢ÙŠØ§Øª")

            except Exception as e:
                print(f"  âŒ Ø®Ø·Ø£: {e}")
                results["failed"] += 1

        return results

    def _extract_chunks(self, pdf_path: Path) -> Generator[dict, None, None]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„ØªÙØ³ÙŠØ± Ù…Ù† PDF."""
        with pdfplumber.open(pdf_path) as pdf:
            current_chunk: dict = {
                "text": "",
                "page_refs": [],
                "raw_verse_hint": "",
            }

            for page in pdf.pages:
                text = page.extract_text(x_tolerance=2, y_tolerance=2)
                if not text:
                    continue

                text = normalize_unicode(text)
                text = self._clean_ocr_artifacts(text)

                for line in text.split("\n"):
                    if self._is_verse_reference(line):
                        if current_chunk["text"].strip():
                            yield current_chunk
                        current_chunk = {
                            "text": line + "\n",
                            "page_refs": [page.page_number],
                            "raw_verse_hint": line,
                        }
                    else:
                        current_chunk["text"] += line + "\n"
                        if page.page_number not in current_chunk["page_refs"]:
                            current_chunk["page_refs"].append(page.page_number)

            if current_chunk["text"].strip():
                yield current_chunk

    def _link_to_verses(self, chunks: list[dict]) -> list[dict]:
        """Ø±Ø¨Ø· ÙƒÙ„ Ù…Ù‚Ø·Ø¹ Ø¨Ø§Ù„Ø¢ÙŠØ© Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø©."""
        linked = []

        for chunk in chunks:
            verse_info = self._resolve_verse(chunk["raw_verse_hint"])
            entry = {
                "text": chunk["text"].strip(),
                "page_refs": chunk["page_refs"],
                "source_note": "Ù…Ù†Ù‚ÙˆÙ„ Ù…Ù† Ø§Ù„Ø´Ø±Ø­ Ø§Ù„Ø´ÙÙ‡ÙŠ â€” Ø®ÙˆØ§Ø·Ø± Ø­ÙˆÙ„ Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ…",
            }

            if verse_info:
                entry["surah_number"] = verse_info.get("surah_number")
                entry["verse_number"] = verse_info.get("verse_number")
                entry["verse_key"] = (
                    f"{verse_info['surah_number']}:{verse_info['verse_number']}"
                )
                entry["linked"] = True
            else:
                entry["linked"] = False

            linked.append(entry)

        return linked

    def _resolve_verse(self, hint: str) -> dict | None:
        """Ù…Ø­Ø§ÙˆÙ„Ø© Ø±Ø¨Ø· ØªÙ„Ù…ÙŠØ­ Ø§Ù„Ù†Øµ Ø¨Ø¢ÙŠØ© Ù…Ø­Ø¯Ø¯Ø©."""
        # Ù…Ø­Ø§ÙˆÙ„Ø© 1: ï´¿...ï´¾
        match = re.search(r"ï´¿(.*?)ï´¾", hint)
        if match:
            verse_text = match.group(1).strip()
            # Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙÙ‡Ø±Ø³
            for key, v in self.verse_index.items():
                if isinstance(v, dict) and verse_text[:20] in str(v.get("text_clean", "")):
                    return v

        # Ù…Ø­Ø§ÙˆÙ„Ø© 2: Ø³ÙˆØ±Ø© X Ø§Ù„Ø¢ÙŠØ© Y
        match = re.search(r"Ø³ÙˆØ±Ø©\s+([\u0600-\u06FF]+)\s*.*?(\d+)", hint)
        if match:
            surah_name = match.group(1)
            verse_num = int(match.group(2))
            for i, name in enumerate(SURAH_NAMES):
                if surah_name in name or name in surah_name:
                    key = f"{i + 1}:{verse_num}"
                    return self.verse_index.get(key)

        # Ù…Ø­Ø§ÙˆÙ„Ø© 3: Ø§Ù„Ø¢ÙŠØ© (N)
        match = re.search(r"Ø§Ù„Ø¢ÙŠØ©\s*\(?(\d+)\)?", hint)
        if match:
            verse_num = int(match.group(1))
            # Ø¨Ø¯ÙˆÙ† Ø³ÙˆØ±Ø© â€” Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø±Ø¨Ø· Ø¨Ø¯Ù‚Ø©
            return None

        return None

    def _is_verse_reference(self, line: str) -> bool:
        """Ù‡Ù„ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø·Ø± ÙŠØ´ÙŠØ± Ù„Ø¨Ø¯Ø§ÙŠØ© ØªÙØ³ÙŠØ± Ø¢ÙŠØ©ØŸ"""
        return any(re.search(p, line) for p in self.VERSE_PATTERNS)

    def _clean_ocr_artifacts(self, text: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø£Ø®Ø·Ø§Ø¡ OCR Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©."""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ù„Ø§ØªÙŠÙ†ÙŠØ© Ø§Ù„Ù…ØªØ·ÙÙ„Ø©
        text = re.sub(r"[a-zA-Z]{1,2}(?=\s|$)", "", text)
        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª
        text = re.sub(r"\s+", " ", text)
        return text.strip()

    def _extract_volume_number(self, pdf_path: Path) -> int:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ù‚Ù… Ø§Ù„Ù…Ø¬Ù„Ø¯ Ù…Ù† Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù."""
        match = re.search(r"(\d+)", pdf_path.stem)
        return int(match.group(1)) if match else 0


async def load_to_database(
    extracted_dir: Path, db_url: str, book_id: int
) -> dict:
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
    import asyncpg

    conn = await asyncpg.connect(db_url)
    stats = {"loaded": 0, "skipped": 0}

    try:
        # Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø¢ÙŠØ§Øª
        rows = await conn.fetch(
            "SELECT id, surah_number, verse_number FROM verses"
        )
        verse_map = {
            f"{r['surah_number']}:{r['verse_number']}": r["id"]
            for r in rows
        }

        for json_file in sorted(extracted_dir.glob("shaarawy_vol_*.json")):
            data = json.loads(json_file.read_text(encoding="utf-8"))

            for chunk in data.get("chunks", []):
                if not chunk.get("linked"):
                    stats["skipped"] += 1
                    continue

                verse_key = chunk.get("verse_key", "")
                verse_id = verse_map.get(verse_key)
                if not verse_id:
                    stats["skipped"] += 1
                    continue

                await conn.execute(
                    """
                    INSERT INTO tafseers (verse_id, book_id, text, page_ref, metadata)
                    VALUES ($1, $2, $3, $4, $5)
                    ON CONFLICT (verse_id, book_id)
                    DO UPDATE SET text = EXCLUDED.text
                    """,
                    verse_id,
                    book_id,
                    chunk["text"],
                    f"Øµ{chunk['page_refs'][0]}" if chunk.get("page_refs") else None,
                    json.dumps({
                        "source_note": chunk.get("source_note", ""),
                        "citation_format": "Ø´ÙÙ‡ÙŠ-Ù…Ø­Ø§Ø¶Ø±Ø©",
                    }),
                )
                stats["loaded"] += 1

    finally:
        await conn.close()

    return stats


def main():
    parser = argparse.ArgumentParser(
        description="ØªØ­ÙˆÙŠÙ„ ØªÙØ³ÙŠØ± Ø§Ù„Ø´Ø¹Ø±Ø§ÙˆÙŠ Ù…Ù† PDF Ø¥Ù„Ù‰ Ù†ØµÙˆØµ Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø¢ÙŠØ§Øª"
    )
    parser.add_argument(
        "--pdf-dir",
        required=True,
        help="Ù…Ø¬Ù„Ø¯ Ù…Ù„ÙØ§Øª PDF",
    )
    parser.add_argument(
        "--output-dir",
        help="Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ (default: data/shaarawy_extracted/)",
    )
    parser.add_argument(
        "--volume",
        type=int,
        help="Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù„Ø¯ ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Ø¹Ø±Ø¶ Ø¨Ø¯ÙˆÙ† ØªÙ†ÙÙŠØ°",
    )
    args = parser.parse_args()

    pipeline = ShaarawyPDFPipeline(
        pdf_dir=args.pdf_dir,
        output_dir=args.output_dir,
    )

    if args.dry_run:
        pdf_files = list(Path(args.pdf_dir).glob("*.pdf"))
        print(f"ğŸ” ÙˆÙØ¬Ø¯ {len(pdf_files)} Ù…Ù„Ù PDF:")
        for f in pdf_files:
            print(f"  - {f.name}")
        return

    results = pipeline.process_all_volumes(volume=args.volume)
    print(f"\n{'=' * 50}")
    print(f"ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
    print(f"   âœ… {results['processed']} Ù…Ø¬Ù„Ø¯ Ù…Ø¹Ø§Ù„Ø¬")
    print(f"   ğŸ“ {results['total_chunks']} Ù…Ù‚Ø·Ø¹ Ø¥Ø¬Ù…Ø§Ù„ÙŠ")
    print(f"   ğŸ”— {results['linked']} Ù…Ø±ØªØ¨Ø· Ø¨Ø¢ÙŠØ§Øª")
    print(f"   âŒ {results['failed']} ÙØ´Ù„")


if __name__ == "__main__":
    main()
